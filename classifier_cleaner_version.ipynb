{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import ECGDataset, read_dataset_from_csv, dataset_folder, read_x_from_csv, read_x_from_mat\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchvision import transforms\n",
    "from utils import smooth_predictions\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "DATA_CSV = 'data_147.csv'\n",
    "DATA_PREFIX = DATA_CSV.split('.')[0]\n",
    "\n",
    "NUM_OF_CLASS = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
    "        super(ECGModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Linear(hidden_dim*2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Apply classifier to every time step\n",
    "        time_steps = lstm_out.shape[1]\n",
    "        out = self.classifier(lstm_out.reshape(-1, lstm_out.shape[2]))\n",
    "        return out.view(-1, time_steps, self.classifier.out_features)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # Create Dataset and DataLoader \n",
    "    # Modify this based on your need \n",
    "    dataset = read_dataset_from_csv(train_csv='data/train/s2_src.csv', label_csv='data/train/s2_lbl.csv')\n",
    "    # dataset = dataset_folder('./data/train')\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "    print(f'Dataset len: {len(dataset)}')\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ECGModel(input_dim=1, hidden_dim=128, num_layers=2, num_classes=NUM_OF_CLASS)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training the model\n",
    "    def train(model, dataloader, criterion, optimizer, num_epochs):\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for inputs, labels in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, 3), labels.view(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            print(f'Epoch {epoch+1}, Average Loss: {total_loss / len(dataloader)}')\n",
    "            torch.save(model.state_dict(), 'ecg_model.pth')\n",
    "\n",
    "    # Example training call\n",
    "    num_epochs = 50\n",
    "    train(model, dataloader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "model = ECGModel(input_dim=1, hidden_dim=128, num_layers=2, num_classes=NUM_OF_CLASS)\n",
    "model.load_state_dict(torch.load('ecg_model.pth'))\n",
    "print(\"Model loaded\")\n",
    "model.eval() \n",
    "# test_dataset = read_dataset_from_csv(train_csv='data/test/s1_src.csv', label_csv='data/test/s1_lbl.csv')\n",
    "test_dataset = read_x_from_csv(f'data/exp_data/{DATA_CSV}')\n",
    "# test_dataset = read_x_from_mat('data/exp_data/61b0.mat')\n",
    "print(\"Dataset loaded\")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "def test(model, dataloader):\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_srcs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Testing\"):\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 2)  # Get predicted class for each time step\n",
    "            all_predictions.extend(predicted.numpy().flatten())\n",
    "            all_labels.extend(labels.numpy().flatten())\n",
    "            all_srcs.extend(inputs.numpy().flatten())\n",
    "\n",
    "    return np.array(all_predictions), np.array(all_labels), np.array(all_srcs)\n",
    "\n",
    "# Run the test\n",
    "predictions, labels, srcs = test(model, test_dataloader)\n",
    "\n",
    "\n",
    "smoothed = smooth_predictions(predictions)\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "sm_acc = accuracy_score(labels, smoothed)\n",
    "\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Accuracy smoothed: {sm_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(2, figsize=(12, 3))\n",
    "start = 3500\n",
    "end = start + 300\n",
    "# axs[0].plot(labels[start:end])\n",
    "# axs[0].set_title('Label')\n",
    "axs[0].plot(smoothed[start:end]*300)\n",
    "axs[0].set_title('Prediction')\n",
    "axs[0].plot(srcs[start:end])\n",
    "axs[0].set_title('ECG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the signal\n",
    "start = 300\n",
    "end = start + 300\n",
    "fontsize = 24\n",
    "signal = srcs[start:end]\n",
    "labels = smoothed[start:end]\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.plot(signal, label='ECG', linewidth=2)\n",
    "colors = {0: 'white', 1: 'lightblue', 2: 'lightgreen'}\n",
    "# Shade each ROI region based on the colors defined\n",
    "segments = np.where(np.diff(labels) != 0)[0]\n",
    "for i in range(len(segments)-1):\n",
    "    \n",
    "    start_x = segments[i]\n",
    "    end_x = segments[i+1]\n",
    "    # start_x = smoothed[start]\n",
    "    # end_x = smoothed[end]\n",
    "    plt.axvspan(start_x, end_x, color=colors[labels[end_x]], alpha=1)\n",
    "\n",
    "\n",
    "# Add labels, grid, and a legend\n",
    "plt.xlabel('Time (s)', fontsize=fontsize)\n",
    "plt.ylabel('Signal amplitude', fontsize=fontsize)\n",
    "plt.title('ECG ROIs classification sample result', fontsize=fontsize)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.xticks(fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "def to_roi_df(labels):\n",
    "    changes = np.diff(labels, prepend=labels[0])\n",
    "\n",
    "    # Identify the start and end indices\n",
    "    starts = np.where(changes != 0)[0]\n",
    "    ends = np.where(changes != 0)[0][1:]\n",
    "\n",
    "    # Include the end of the last segment\n",
    "    ends = np.append(ends, len(labels))\n",
    "\n",
    "    # Get the wave types\n",
    "    waves = labels[starts]\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'wave': waves,\n",
    "        'start_index': starts,\n",
    "        'end_index': ends - 1\n",
    "    })\n",
    "\n",
    "    # Filter out the segments with wave type 0\n",
    "    df = df[df['wave'] != 0].reset_index(drop=True)\n",
    "\n",
    "    # Map the wave types to their names\n",
    "    wave_mapping = {2: 'p', 1: 'qrs'}\n",
    "    df['wave'] = df['wave'].map(wave_mapping)\n",
    "    df['duration'] = df['end_index'] - df['start_index']\n",
    "    return df\n",
    "\n",
    "def find_peaks_in_segment(segment, wave_type):\n",
    "    plt.show()\n",
    "    if wave_type == 'p':\n",
    "        # For 'p' wave, we expect one peak\n",
    "        peaks, _ = find_peaks(segment)\n",
    "        if peaks.size == 0:\n",
    "            return [np.nan]  # No peak found\n",
    "        return [peaks[np.argmax(segment[peaks])]]  # Return the location of the highest peak\n",
    "    elif wave_type == 'qrs':\n",
    "        # For 'qrs' wave, we expect two peaks: R and J\n",
    "        peaks, _ = find_peaks(segment)\n",
    "        \n",
    "        if peaks.size < 2:\n",
    "            return [np.nan, np.nan]  # Less than 2 peaks found\n",
    "        sorted_peaks = peaks[np.argsort(segment[peaks])][::-1]  # Sort peaks by height\n",
    "        return sorted(sorted_peaks[:2])  # Return the two largest peaks in time order\n",
    "\n",
    "# Function to get peaks from the DataFrame\n",
    "def get_peaks_from_roi(df, signal):\n",
    "    peaks_data = {'wave': [], 'peak1': [], 'peak2': [], 'duration':[]}\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        segment = signal[row['start_index']:row['end_index'] + 1]\n",
    "\n",
    "        peaks = find_peaks_in_segment(segment, row['wave'])\n",
    "        print(peaks)\n",
    "        peaks_data['wave'].append(row['wave'])\n",
    "        peaks_data['peak1'].append(row['start_index'] + peaks[0] if not np.isnan(peaks[0]) else np.nan)\n",
    "        print(row['start_index'] + peaks[0] if not np.isnan(peaks[0]) else np.nan)\n",
    "        peaks_data['peak2'].append(row['start_index'] + peaks[1] if len(peaks) > 1 and not np.isnan(peaks[1]) else np.nan)\n",
    "        peaks_data['duration'].append(row.duration)\n",
    "\n",
    "    return pd.DataFrame(peaks_data)\n",
    "\n",
    "\n",
    "\n",
    "def filter_peaks(df):\n",
    "    valid_indices = []\n",
    "    i = 0\n",
    "    while i < len(df) - 1:\n",
    "        if (df.iloc[i]['wave'] == 'p' and not np.isnan(df.iloc[i]['peak1']) and\n",
    "            df.iloc[i + 1]['wave'] == 'qrs' and not np.isnan(df.iloc[i + 1]['peak1']) and not np.isnan(df.iloc[i + 1]['peak2'])):\n",
    "            valid_indices.extend([i, i + 1])\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1\n",
    "    return df.iloc[valid_indices].reset_index(drop=True)\n",
    "\n",
    "roi_df = to_roi_df(smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_df = get_peaks_from_roi(roi_df, srcs)\n",
    "peak_df = filter_peaks(peak_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak_df.to_csv(f'{DATA_PREFIX}_peaks.csv')\n",
    "peak_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_intervals(df, sampling_rate=512):\n",
    "    intervals = {\n",
    "        'p_peak_index': [],\n",
    "        'pr_interval': [],\n",
    "        'pj_interval': [],\n",
    "        'duration':[]\n",
    "    }\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(df) - 1:\n",
    "        if (df.iloc[i]['wave'] == 'p' and not np.isnan(df.iloc[i]['peak1']) and\n",
    "            df.iloc[i + 1]['wave'] == 'qrs' and not np.isnan(df.iloc[i + 1]['peak1']) and not np.isnan(df.iloc[i + 1]['peak2'])):\n",
    "            p_peak_index = df.iloc[i]['peak1']\n",
    "            qrs_peak1 = df.iloc[i + 1]['peak1']\n",
    "            qrs_peak2 = df.iloc[i + 1]['peak2']\n",
    "            \n",
    "            pr_interval = (qrs_peak1 - p_peak_index) / sampling_rate\n",
    "            pj_interval = (qrs_peak2 - p_peak_index) / sampling_rate\n",
    "            print(f'({qrs_peak2} - {p_peak_index})/{sampling_rate} = {pj_interval}')\n",
    "            print(pj_interval)\n",
    "            \n",
    "            intervals['p_peak_index'].append(p_peak_index)\n",
    "            intervals['pr_interval'].append(pr_interval)\n",
    "            intervals['pj_interval'].append(pj_interval)\n",
    "            intervals['duration'].append(df.iloc[i].duration)\n",
    "            \n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return pd.DataFrame(intervals)\n",
    "\n",
    "intervals = calculate_intervals(peak_df)\n",
    "intervals['p_peak_index'] = intervals['p_peak_index'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "intervals.set_index('p_peak_index', inplace=True)\n",
    "ecg_df = pd.DataFrame({\"ECG\":srcs, \"Label\":smoothed})\n",
    "merged_df = ecg_df.merge(intervals, right_on='p_peak_index', left_index=True, how='left')\n",
    "merged_df = merged_df.ffill().bfill()\n",
    "merged_df.to_csv(f'data/intervals/{DATA_PREFIX}_intervals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df = peak_df[peak_df['wave'] == 'p']\n",
    "p_df['index'] = [int(x) for x in p_df['peak1']]\n",
    "p_df['ppinterval'] = p_df['peak1'].diff()\n",
    "p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.merge(p_df[['index', 'ppinterval']], right_on='index', left_index=True, how='left')\n",
    "merged_df = merged_df.ffill().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(f\"{DATA_PREFIX}_intervals.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "painstudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
